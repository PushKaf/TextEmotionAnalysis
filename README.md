# Text Emotion Analysis

This is just the font end for my machine and deep learning models. This includes a model.py which basically replicates the preprocessing I did for the data and loads the models.  

The models were trained on a Twitter dataset found on Kaggle. 
The BiLSTM model was a Sequential model with the following layers: 
- Embedding
- Bidirectional LSTM model with 32 units, a dropout of 0.6, kernel, and recurrent l2 regularization of 0.001. 
- BatchNormalization default
- Dropout .6
- Dense layer with 6 units with softmax activation.
  
This model used the Adam optimizer with 0.01 lr. This also utilized the sparse categorical cross-entropy loss function with 15 epochs and a batch size 64. 

The finalData.csv was also generated by combining the 3 CSV files. 
